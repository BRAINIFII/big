Desc:
Apache Spark is an open-source, distributed computing system for big data processing. It is designed to perform both batch processing and real-time analytics. Spark can handle large data sets and provides an interface for programming entire clusters with implicit data parallelism and fault-tolerance. It can be used in a variety of applications such as SQL, streaming, and machine learning, and can be integrated with Hadoop and other big data tools.

Scala is a programming language that runs on the Java Virtual Machine (JVM). It is a modern, object-oriented and functional programming language. Scala combines the object-oriented and functional programming paradigms. It supports the seamless integration of object-oriented and functional language constructs. Scala is used for a wide range of applications, including data processing, machine learning, and web development, and is particularly well-suited for big data processing using Apache Spark.

Scala has built-in error handling mechanisms, such as try-catch and try-finally blocks, similar to Java. It also provides the option to use the Either and Option types to handle errors and exceptional cases. The Either type can be used to represent values with two possible types, one for success and one for failure. The Option type can be used to represent the presence or absence of a value. In addition, Scala's for-comprehension can be used for error handling in a more functional way by chaining together multiple computations that may fail, and handling the errors at the end.

# switch to Hadoop user
# Install scala-sbt via the latest command available in the terminal

echo "deb https://repo.scala-sbt.org/scalasbt/debian all main" | sudo tee /etc/apt/sources.list.d/sbt.list
echo "deb https://repo.scala-sbt.org/scalasbt/debian /" | sudo tee /etc/apt/sources.list.d/sbt_old.list
curl -sL "https://keyserver.ubuntu.com/pks/lookup?op=get&search=0x2EE0EA64E40A89B84B2DF73499E82A75642AC823" | sudo apt-key add

sudo apt-get update
sudo apt-get install sbt

# enter sbt in the terminal to verify installation
# create the scala program for exception handling
> nano ExceptionHandlingTest.scala
# paste code

> nano exceptionhandlingtest.sbt
# create the sbt dependency file ‘exceptionhandlingtest.sbt’
#paste code

# start spark master & worker
> spark-master.sh
> start-worker.sh spark://<ip-address>:7077/

# go back to the terminal pointing to the directory with scala and sbt file and create the sbt package
> sbt package


> spark-submit --class ExceptionHandlingTest.class --master spark://<ip-address>:7077/ \target/scala-2.12/exceptionhandlingtest_2.12-1.0.jar